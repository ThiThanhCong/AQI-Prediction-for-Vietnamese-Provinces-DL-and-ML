\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[vietnamese]{babel}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Conference Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Cao Hoai Sang}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
        \textit{name of organization (of Aff.)}\\
        Ho Chi Minh City, Viet Nam \\
        21522541@gm.uit.edu.vn}
    \and
    \IEEEauthorblockN{2\textsuperscript{nd} Nguyen Tran Gia Kiet}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
        \textit{name of organization (of Aff.)}\\
        City, Country \\
        email address or ORCID}
    \and
    \IEEEauthorblockN{3\textsuperscript{rd} Thi Thành Công}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
        \textit{name of organization (of Aff.)}\\
        City, Country \\
        email address or ORCID}
    \and
    \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
        \textit{name of organization (of Aff.)}\\
        City, Country \\
        email address or ORCID}
    \and
    \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
        \textit{name of organization (of Aff.)}\\
        City, Country \\
        email address or ORCID}
    \and
    \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
        \textit{name of organization (of Aff.)}\\
        City, Country \\
        email address or ORCID}
}

\maketitle

\begin{abstract}
    This document is a model and instructions for \LaTeX.
    This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
    or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
    component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
This document is a model and instructions for \LaTeX.
Please observe the conference page limits.

\section{Nghiên cứu liên quan}

\subsection{Gauss-Newton nonlinear method}

Vào năm 2015, ứng dụng dự báo phục hồi sau xuất viện của Phạm Thị Hương được
sử dụng trong luận văn thạc sĩ khoa học [1]. Sử dụng phương pháp Gauss-Newton
phi tuyến để ước lượng giá trị nhỏ nhất của bình phương sai số. \cite{b1}



\section{Tài nguyên}
\section{Phương pháp luận}
\subsection{Gauss newton method nonlinear}
\subsubsection{Least Squares}
The distance between a fitted curve and an observation is called a residual, or an error.

\begin{center}
    $Residuals = y_i - \widehat{y}_i$
\end{center}

The sum of squared errors is calculated by the following equation:

\begin{center}
    $SSE = \sum_{i=1}^{n}(y_i - \widehat{y}_i)^2$
\end{center}

Where: \\
\indent\textbullet\ \(y_i\) is observed values\\
\indent\textbullet\ \(\widehat{y}_i\) is fitted values\\

\subsubsection{Newton's method}
With the function \( y = y_0e^{-kt} \) we find the mininum of SSE. we find the \(k\) value with Newton's method

\begin{center}
    $SSE = \sum_{i}^{n}(y_i-y_0e^{-kt_i})^2$
\end{center}


\begin{center}

    \(k_{new} = k_{old} - \frac{f'(k_{old})}{f''(k_{old})}\)

\end{center}

Or we can explain by Hessan matrix like this:

\begin{center}
    \[
        \begin{pmatrix}
            k_{\text{new}} \\ y_{0, \text{new}}
        \end{pmatrix} =
        \begin{pmatrix}
            k_{\text{old}} \\ y_{0, \text{old}}
        \end{pmatrix} - H^{-1}G
    \]
\end{center}

Where: \\
\indent\textbullet\ \(H = \begin{bmatrix}
    \frac{\partial^2 f}{\partial k} & \frac{\partial^2 f}{\partial k \partial y_0}
    \\ \frac{\partial^2 f}{\partial y_0}  & \frac{\partial^2 f}{\partial y_0 \partial k}
\end{bmatrix}\)
\indent\textbullet\ \(G = \begin{bmatrix}
    \frac{\partial f}{\partial k} \\ \frac{\partial f}{\partial y_0}
\end{bmatrix}\)\\

The problem with Neuton's method in nonlinear regression is the Hessian and its inverse are problematic to calculate. To solve this problem, the Gauss-Newton method instead approximates the Hessian.

We can rewrite the SSE like this:
\begin{center}
    $SSE = \sum_{i}^{n}r_i^2 = r^Tr$
\end{center}

Where: \\
\indent\textbullet\ \(r\) represents a vector with the residuals \\

\subsubsection{Gauss Newton method}
We differentiate SSE respect to the parameters in the model with the chain rule, we obtain the following equation.
\begin{center}

    \(
    {\frac{\partial SSE}{\partial \beta_j}} = 2\sum_{i}^{n}{r_i.\frac{\partial r_i}{\partial\beta_j}}
    \)

\end{center}
Then, remove two number because it will not affect the estimation of the parameters. Corresponds to the Jacobian matrix.


\begin{center}

    \(
    J_r = \begin{bmatrix}
        \frac{\partial r_1}{\partial \beta_1} &  & \frac{\partial r_1}{\partial \beta_2} \\
        \frac{\partial r_2}{\partial \beta_1} &  & \frac{\partial r_2}{\partial \beta_2} \\
        \vdots                                &  & \vdots                                \\
        \frac{\partial r_n}{\partial \beta_1} &  & \frac{\partial r_n}{\partial \beta_2}
    \end{bmatrix}
    \)

\end{center}

With the following equation for the sum of squared errors (SSE):
\[
    SSE = \sum_{i=1}^{n} (y_i - y_0 e^{-kt_i})^2
\]
We have:
\begin{center}

    \(
    \frac{\partial^2 {SSE}}{\partial B_j \partial \beta_k} = \sum_{i}^{n}(\frac{\partial r_i}{\beta_j}\frac{\partial r_i}{\beta_k} + r_i\frac{\partial^2 r_i}{\partial \beta_j \partial \beta_k})
    \)

\end{center}
The main difference between Newton's method and Gauss-Newton is that the Gauss-Newton method neglects \(r_i\frac{\partial^2 r_i}{\partial \beta_j \partial \beta_k}\)
So that the second derivative is approximated by the following function
\begin{center}

    \(
    \frac{\partial^2 {SSE}}{\partial B_j \partial \beta_k} \approx \sum_{i}^{n}(\frac{\partial r_i}{\beta_j}\frac{\partial r_i}{\beta_k}) = J^T_rJ_r
    \)

\end{center}

Using the following updating  role in Newton's method.  For Gauss-Newton simplying plug in the approximation for the Hessian matrix and for the gradient.

\begin{center}
    \[
        \begin{pmatrix}
            k_{\text{new}} \\ y_{0,\text{new}}
        \end{pmatrix} =
        \begin{pmatrix}
            k_{\text{old}} \\ y_{0,\text{old}}
        \end{pmatrix} - (J^T_r J_r)^{-1} J^T_r r
    \]
\end{center}

With \(\beta\) as a column vector with the parameters that are estimated. For a simple example where we only estimate two parameters, the equation looks like this:

\begin{center}
    \[
        \beta_{\text{new}} = \beta_{\text{old}} - (J^T_r J_r)^{-1} J^T_r r(\beta_{\text{old}})
    \]
    \[
        \begin{pmatrix}
            k_{\text{new}} \\ y_{0,\text{new}}
        \end{pmatrix} =
        \begin{pmatrix}
            k_{\text{old}} \\ y_{0,\text{old}}
        \end{pmatrix} - (J^T_r J_r)^{-1} J^T_r r \begin{pmatrix}
            k_{\text{old}} \\ y_{0,\text{old}}
        \end{pmatrix}
    \]
\end{center}


\begin{thebibliography}{00}
    \bibitem{b1}  P. T. Huong, "Linear regression, polynomial regression, and applications,
    master’s thesis in science," 2015.
\end{thebibliography}

\end{document}
